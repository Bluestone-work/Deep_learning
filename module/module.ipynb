{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = tensor([[0.1115, 0.0936, 0.5347, 0.7928, 0.4501, 0.1781, 0.5557, 0.7199, 0.1021,\n",
      "         0.1192, 0.5246, 0.5367, 0.6979, 0.8654, 0.3845, 0.6928, 0.4024, 0.5351,\n",
      "         0.2625, 0.3731],\n",
      "        [0.0714, 0.7706, 0.0724, 0.0040, 0.7932, 0.1146, 0.3881, 0.7451, 0.0387,\n",
      "         0.2628, 0.0313, 0.9623, 0.4777, 0.9536, 0.7158, 0.5966, 0.2552, 0.3952,\n",
      "         0.0776, 0.8718]]),-----result = tensor([[-0.0594,  0.2220,  0.1441,  0.1463,  0.0980,  0.0654, -0.0687, -0.0290,\n",
      "         -0.1378,  0.3071],\n",
      "        [ 0.1553,  0.3091,  0.1123,  0.1712,  0.2280,  0.1172, -0.0194,  0.0481,\n",
      "         -0.0548,  0.2807]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#第一种，简单的网络定义\n",
    "net = nn.Sequential(nn.Linear(20,256) , nn.ReLU() , nn.Linear(256 , 10))\n",
    "X = torch.rand(2 , 20)\n",
    "result =  net(X)\n",
    "print('X={},-----result={}'.format(X,result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X=tensor([[0.5041, 0.3026, 0.4379, 0.1305, 0.3008, 0.1753, 0.8016, 0.6705, 0.6735,\n",
      "         0.3611, 0.9331, 0.3823, 0.3648, 0.3714, 0.4860, 0.6378, 0.4694, 0.1986,\n",
      "         0.6714, 0.3571],\n",
      "        [0.6927, 0.0013, 0.5385, 0.3272, 0.6236, 0.7863, 0.7736, 0.3076, 0.8692,\n",
      "         0.1073, 0.7720, 0.7887, 0.1069, 0.5550, 0.1704, 0.9198, 0.0768, 0.3074,\n",
      "         0.4011, 0.9153]]),-----result=tensor([[-0.0091, -0.2452,  0.1301,  0.0153, -0.1412,  0.1283, -0.0668, -0.1255,\n",
      "         -0.1989,  0.1961],\n",
      "        [-0.0758, -0.2136,  0.1751,  0.0883, -0.1017,  0.1253, -0.1405, -0.1603,\n",
      "         -0.1712,  0.0886]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#第二种\n",
    "class MPL(nn.Module):\n",
    "    def __init__(self,input,hidden,output, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.fc1 = nn.Linear(input , hidden)\n",
    "        self.fc2 = nn.Linear(hidden , output)\n",
    "    \n",
    "    def forward(self , X):\n",
    "        X = F.relu(self.fc1(X))\n",
    "        X = self.fc2(X)\n",
    "        return X\n",
    "\n",
    "net = MPL(20,256,10)\n",
    "X = torch.rand(2 , 20)\n",
    "result = net(X)\n",
    "print('X={},-----result={}'.format(X,result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X=tensor([[0.2237, 0.7469, 0.4308, 0.0592, 0.0507, 0.7074, 0.8138, 0.7027, 0.4568,\n",
      "         0.4982, 0.3072, 0.4822, 0.7691, 0.4516, 0.2072, 0.1888, 0.9977, 0.4373,\n",
      "         0.7154, 0.8857],\n",
      "        [0.2872, 0.5146, 0.8375, 0.0794, 0.5652, 0.3961, 0.7796, 0.4764, 0.3326,\n",
      "         0.9264, 0.2436, 0.3067, 0.2138, 0.3720, 0.5062, 0.7011, 0.1849, 0.7846,\n",
      "         0.0020, 0.3256]]),-----result=tensor([[ 0.2110, -0.1703,  0.1738,  0.0195, -0.1321,  0.0840,  0.0603, -0.0723,\n",
      "         -0.0482,  0.2277],\n",
      "        [ 0.0354, -0.2034,  0.1796,  0.0383, -0.1457,  0.1070,  0.0889, -0.1897,\n",
      "         -0.1188,  0.1034]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    # 用模型参数声明层。这里，我们声明两个全连接的层\n",
    "    def __init__(self):\n",
    "        # 调用MLP的父类Module的构造函数来执行必要的初始化。\n",
    "        # 这样，在类实例化时也可以指定其他函数参数，例如模型参数params（稍后将介绍）\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20, 256)  # 隐藏层\n",
    "        self.out = nn.Linear(256, 10)  # 输出层\n",
    "\n",
    "    # 定义模型的前向传播，即如何根据输入X返回所需的模型输出\n",
    "    def forward(self, X):\n",
    "        # 注意，这里我们使用ReLU的函数版本，其在nn.functional模块中定义。\n",
    "        return self.out(F.relu(self.hidden(X)))\n",
    "\n",
    "X = torch.rand(2 , 20)\n",
    "result = net(X)\n",
    "print('X={},-----result={}'.format(X,result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X=tensor([[0.0460, 0.0404, 0.1112, 0.5700, 0.5512, 0.8027, 0.5400, 0.6199, 0.5469,\n",
      "         0.9850, 0.3960, 0.1322, 0.6344, 0.3549, 0.8300, 0.9747, 0.2821, 0.7437,\n",
      "         0.4151, 0.3981],\n",
      "        [0.9099, 0.1291, 0.1303, 0.4802, 0.8547, 0.3972, 0.2298, 0.5309, 0.2225,\n",
      "         0.1699, 0.6550, 0.5433, 0.9412, 0.8245, 0.9975, 0.0017, 0.6652, 0.3298,\n",
      "         0.6722, 0.2402]]),-----result=tensor([[-0.3766, -0.1083, -0.0013,  0.0719,  0.0811,  0.0489, -0.0946, -0.0973,\n",
      "         -0.1130, -0.0071],\n",
      "        [-0.1864, -0.0938,  0.1363,  0.1090,  0.0251,  0.1845, -0.0476, -0.1472,\n",
      "          0.0021,  0.0651]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#第三种，使用顺序块\n",
    "class MySequential(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        for idx, module in enumerate(args):\n",
    "            # 这里，module是Module子类的一个实例。我们把它保存在'Module'类的成员\n",
    "            # 变量_modules中。_module的类型是OrderedDict\n",
    "            self._modules[str(idx)] = module\n",
    "\n",
    "    def forward(self, X):\n",
    "        # OrderedDict保证了按照成员添加的顺序遍历它们\n",
    "        for block in self._modules.values():\n",
    "            X = block(X)\n",
    "        return X\n",
    "\n",
    "net = MySequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\n",
    "net(X)\n",
    "X = torch.rand(2 , 20)\n",
    "result = net(X)\n",
    "print('X={},-----result={}'.format(X,result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0543, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FixedHiddenMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 不计算梯度的随机权重参数。因此其在训练期间保持不变\n",
    "        self.rand_weight = torch.rand((20, 20), requires_grad=False)#放进一个随机的权重在里面，但是他不会参与计算\n",
    "        self.linear = nn.Linear(20, 20)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.linear(X)\n",
    "        # 使用创建的常量参数以及relu和mm函数\n",
    "        X = F.relu(torch.mm(X, self.rand_weight) + 1)\n",
    "        # 复用全连接层。这相当于两个全连接层共享参数\n",
    "        X = self.linear(X)\n",
    "        # 控制流\n",
    "        while X.abs().sum() > 1:\n",
    "            X /= 2\n",
    "        return X.sum()\n",
    "\n",
    "net = FixedHiddenMLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0651, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NestMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(20, 64), nn.ReLU(),\n",
    "                                 nn.Linear(64, 32), nn.ReLU())\n",
    "        self.linear = nn.Linear(32, 16)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.linear(self.net(X))\n",
    "\n",
    "chimera = nn.Sequential(NestMLP(), nn.Linear(16, 20), FixedHiddenMLP())\n",
    "chimera(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
