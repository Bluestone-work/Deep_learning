![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20241002235002.png)
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20241002235008.png)
存在一堆key value pair，先用query和每一个key做一个α的attention score function，丢到softmax中，在和每一个value做加权的和，最终就得到每一个的输出。

## 拓展到高纬度
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20241002235259.png)
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20241002235938.png)
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20241003010932.png)
## additive attention
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20241003011233.png)
K和Wk相乘会得到一个长为h的向量，Q和Wq相乘也会得到一个长为h的向量。 再把它加起来，加完之后放到一个tanh的向量里面会成为一个长为h的向量，然后在和vT相乘就变成一个值
等价于将key和value合并起来之后放入一个隐藏大小为h，输出大小为1的单隐藏层MLP。
好处在于你的key和value可以是任意的长度。
## scaled Dot-product Attention
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20241003011733.png)
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20241003011742.png)
第i行就表示第i个qurey他的权重是什么样的。
## 总结
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20241003012042.png)
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20241003012049.png)
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20241003012103.png)
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20241003012121.png)
```
class AdditiveAttention(nn.Module):

    """加性注意力"""

    def __init__(self, key_size, query_size, num_hiddens, dropout, **kwargs):

        super(AdditiveAttention, self).__init__(**kwargs)

        self.W_k = nn.Linear(key_size, num_hiddens, bias=False)#key_size 和 query_size 分别是键（key）和查询（query）的向量维度大小。num_hiddens 是注意力机制中的隐藏层维度大小。

        self.W_q = nn.Linear(query_size, num_hiddens, bias=False)#W_k 和 W_q 是两个线性变换，分别用于将键（key）和查询（query）投影到一个统一的隐藏空间。

        self.w_v = nn.Linear(num_hiddens, 1, bias=False)#w_v 是另一个线性变换，用于将加性注意力得出的特征映射到一个标量分值。

        self.dropout = nn.Dropout(dropout)#dropout 用于防止过拟合，通过随机丢弃部分神经元。

  

    def forward(self, queries, keys, values, valid_lens):##valid_lens对每个value我应该考虑前多少个。

        queries, keys = self.W_q(queries), self.W_k(keys)#通过 self.W_q(queries) 和 self.W_k(keys)，查询和键分别经过线性变换投射到一个相同的隐藏空间，大小为 num_hiddens。

        # 在维度扩展后，

        # queries的形状：(batch_size，查询的个数，1，num_hidden)

        # key的形状：(batch_size，1，“键－值”对的个数，num_hiddens)

        # 使用广播方式进行求和

        features = queries.unsqueeze(2) + keys.unsqueeze(1)#unsqueeze()函数起升维的作用,参数表示在哪个地方加一个维度。

        features = torch.tanh(features)

        # self.w_v仅有一个输出，因此从形状中移除最后那个维度。

        # scores的形状：(batch_size，查询的个数，“键-值”对的个数)

        scores = self.w_v(features).squeeze(-1)#self.w_v(features) 将上述特征映射到一个标量，这个标量代表某个查询与键的相关性分数。通过 squeeze(-1) 去掉最后一个维度，得到 scores，其形状为 (batch_size，查询的个数，“键-值”对的个数)。

        self.attention_weights = masked_softmax(scores, valid_lens)#masked_softmax(scores, valid_lens) 是对 scores 进行 softmax 操作，同时考虑有效长度 valid_lens 进行掩码处理，得到归一化的注意力权重 self.attention_weights。

        # values的形状：(batch_size，“键－值”对的个数，值的维度)

        return torch.bmm(self.dropout(self.attention_weights), values)#最后通过 torch.bmm（批量矩阵乘法），注意力权重与值 values 相乘，得到加权后的输出。
```
### 1. `features = queries.unsqueeze(2) + keys.unsqueeze(1)`

这一行代码的作用是通过将**查询（queries）**和**键（keys）**进行相加，得到特征 `features`，其中涉及到张量维度的扩展和广播机制。

#### a. 张量维度解释：

- `queries` 和 `keys` 是经过线性变换后的张量，形状分别是：
    - `queries`: `(batch_size, query_len, num_hiddens)`，即批量大小、查询的长度（或查询的个数）以及隐藏层维度。
    - `keys`: `(batch_size, key_len, num_hiddens)`，即批量大小、键的长度（或键-值对的个数）以及隐藏层维度。

#### b. `unsqueeze` 的作用：

- `unsqueeze(2)` 和 `unsqueeze(1)` 是 PyTorch 中扩展张量维度的操作。
    - `queries.unsqueeze(2)` 将 `queries` 的形状变为 `(batch_size, query_len, 1, num_hiddens)`，在第 2 维度（索引为 2 的位置）添加一个大小为 1 的新维度。
    - `keys.unsqueeze(1)` 将 `keys` 的形状变为 `(batch_size, 1, key_len, num_hiddens)`，在第 1 维度添加一个大小为 1 的新维度。

#### c. 广播机制：

- 现在 `queries.unsqueeze(2)` 的形状是 `(batch_size, query_len, 1, num_hiddens)`，`keys.unsqueeze(1)` 的形状是 `(batch_size, 1, key_len, num_hiddens)`。
- 通过广播机制，PyTorch 可以自动扩展这两个张量，使它们在最后两个维度上进行逐元素相加，生成新的 `features` 张量，其形状为 `(batch_size, query_len, key_len, num_hiddens)`。这相当于对每个查询和每个键之间进行逐个相加操作，生成特征组合。

### 2. `features = torch.tanh(features)`

在这一行中，对生成的特征 `features` 进行非线性激活。

#### a. `torch.tanh` 解释：

- `tanh` 是双曲正切函数，它是一种常用的激活函数，将输入张量的值映射到区间 `(-1, 1)`。
- 在这里，`tanh` 的作用是引入非线性，使得查询和键之间的相加操作结果经过非线性变换，从而提升模型的表达能力。

#### b. 特征矩阵 `features` 的形状：

经过 `tanh` 激活后，`features` 的形状仍然是 `(batch_size, query_len, key_len, num_hiddens)`。

### 3. `scores = self.w_v(features).squeeze(-1)`

这一行代码是将之前得到的特征 `features` 映射到一个注意力分数（scores），并调整形状以便后续计算。

#### a. `self.w_v(features)` 作用：

- `self.w_v` 是一个线性变换层，定义为 `nn.Linear(num_hiddens, 1, bias=False)`，它的作用是将 `features` 从 `num_hiddens` 维度映射到一个单一的分数（标量）。
- `features` 的形状为 `(batch_size, query_len, key_len, num_hiddens)`，通过线性变换 `self.w_v(features)`，我们将每个 `num_hiddens` 维度的向量映射为一个标量。这样 `features` 的形状变为 `(batch_size, query_len, key_len, 1)`。

#### b. `squeeze(-1)` 作用：

- `squeeze(-1)` 的作用是移除最后一个维度中大小为 1 的维度。`features` 的形状从 `(batch_size, query_len, key_len, 1)` 变为 `(batch_size, query_len, key_len)`。
- 这个操作的目的是简化张量的形状，以便后续计算注意力权重。

### 4. `scores` 的形状解释：

最终生成的 `scores` 的形状是 `(batch_size, query_len, key_len)`，它表示每个**查询**与每个**键**之间的匹配分数或相关性得分：

- `batch_size`: 表示批量大小。
- `query_len`: 表示查询的个数（即每个批次中有多少个查询向量）。
- `key_len`: 表示键的个数（即每个批次中有多少个键值对）。

这些 `scores` 将用于后续的**注意力权重计算**，例如通过 softmax 归一化后，结合 `values` 得到最终的注意力输出。

### 小结

- **查询和键的组合**：通过 `unsqueeze` 操作扩展维度，使用广播机制将每个查询与每个键相加，得到特征组合。
- **非线性激活**：通过 `tanh` 激活函数，使得查询和键的组合引入非线性，增强模型的表示能力。
- **线性变换得到分数**：通过线性层 `self.w_v` 将特征组合映射为一个标量（注意力得分），并通过 `squeeze` 简化张量形状。
- **结果**：得到的 `scores` 是每个查询与所有键的匹配分数，用于后续的注意力权重计算。

![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20241003160833.png)
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20241003160847.png)
## mask_softmax()
这个函数并不是说把注意力低的过滤掉，是把填充部分的信息给过滤掉，比如说需要的句子长度是10但是我这个句子只有4个词，所以我需要pad6个词上去，那么我现在拿一个qurey，我要去查询说我怎么翻译某一个词的话那么我去pay attention的时候那些key-value就使用key-value pair。我应该就只看前四个，不看后面6个，因为我知道后面6个是没意义的。