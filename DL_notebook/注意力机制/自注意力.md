主要是抉择key-vcalue-query怎么选。
## 自注意力
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20241004154622.png)
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20241004154657.png)
	xi是一个长为d的向量（长为d的向量指的是对原有序列进行裁剪以后得到的输入）
	假设每个序列有一个长为d的特征，自注意力将xi既当key又当value还当query，这样来对每个序列抽取特征得到y1到yn。
	这么做可以使得我不需要额外的key，value，query，那么我们就可以用自注意力这个东西，来处理序列，不同于我们之前需要encode，decode。

## CNN,RNN,自注意力对比

![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20241004155353.png)
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20241004155400.png)
CNN做法：把一个序列当作是一个一维的输入，我们之前的图片有高和宽，每个像素还有一个channel数。我们用1d卷积把他卷积进去，每个向量元素的特征就当作channel，他就可以用来处理文本序列。（相当于把文本视为图片，只不过是1维图片，用1D卷积核进行处理而已。文本embedding的那些特征视为CNN中的channel。这样就可以处理了）

他的计算复杂度：k是窗口的大小，每一次看一个长为k的序列，在×长度在×d的平方d为dimension

自注意力是越长的句子，越复杂，计算就越复杂。计算某一个xi的时候是和别的向量无关的。

## 位置编码
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20241004174051.png)
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20241004174058.png)
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20241004174412.png)
奇数列是一个sin函数，偶数列是一个cos函数，不同列的周期不一样。
在处理词元序列时，循环神经网络是逐个的重复地处理词元的， 而自注意力则因为并行计算而放弃了顺序操作。 为了使用序列的顺序信息，通过在输入表示中添加 _位置编码_（positional encoding）来注入绝对的或相对的位置信息。 位置编码可以通过学习得到也可以直接固定得到。
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20241004233646.png)
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20241004233709.png)
## 位置编码矩阵
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20241004174452.png)
第6维到第9维
横轴是i，每条线表示一个j。
每个 token 的  长度为 d 的 特征向量的 6 7 8 9 个元素

## 绝对位置编码
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20241004233130.png)
可以有任意的长度来进行编码。
对序列中的第i个样本给一个独一无二的长为d的数据信息，给他加到状态里作为输入。

## 相对位置编码
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20241004233439.png)
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20241004233447.png)

![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20241004233915.png)
    首先对于编码我们自然而然能够想到二进制编码，这也是书中讨论的，在二进制编码中，随着数的增加，最低位(最右边)的二进制位一定是变化次数最多的，因为每隔一个数字他就会跳变一次。同理每隔两个数字，倒数第二位就会变一次，每个四个数字，倒数第三位变一次。可以看到在二进制编码中，存在不同位的变化频率不同的规律。
  再回到三角函数编码中，有n个序列，每个序列值的维度是d，三角函数编码每行随着d中2j的增大，频率会变低。也就是说，对于同一序列(三角编码的一行)，越靠后的改变频率越低。即用三角编码的一行的不同位置模拟了二进制编码(模拟了不同位频率不同的特点)。
  最后是P矩阵的热图(这里我的理解可能有些问题，可指正)，可以看到越靠左和靠下颜色越深越密集。靠下我是这么理解的:对于一个长序列，越靠后的在网络中留下的信息越多(离结果近所以记得牢),即靠下的序列权重更大。   然后越靠左信息越多这个我不还是不太明白为什么。