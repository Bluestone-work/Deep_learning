![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20240927174729.png)
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20240927174737.png)
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20240927174743.png)
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20240927174750.png)

## 1.使用计数来建模
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20240927175108.png)
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20240927210306.png)
n是token的个数，x是token，p是x在整个n中出现的概率

## N元语法
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20240927210547.png)
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20240927210623.png)
假设tau = 0，每次算的时候都不用管前面的状态。认为每个都是独立的。
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20240927212428.png)
最大的好处是可以处理比较长的序列，
n元语法的话，给一个任意长的序列，我要看的子序列是固定的，比如说二元语法，我每次只看长为2的序列，我能够把所有的n，x1，x2是任何一个词的代表，全部给存下来，假设有1000个词，长为2的可能性有1000 * 1000存1000000个 可能性，然后把每一个词和另外一个词在文本中出现的概率全部×起来那么就是存1000000个数，然后把一个词出现的概率给存下来，也就是n(x1)，1000个数，然后再存个n，那么下次查询任何一个任意长的序列我的时间复杂度，那就是O(t),O(t)中的t是序列的长度。
3元长度也是一样的。这个和n元语法，这之中的n是一个指数关系。当你的n变得很长的时候，要存的东西就会变的很大，所以会带来一个问题。一般来说用的就是二元或是三元语法。
总结一下，n越大精度越高，但是空间复杂度也会相应变高。
## 总结
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20240927221144.png)
 ![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20240927222036.png)
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20240927222055.png)


## 当序列变得太长而不能被模型一次性全部处理时， 我们可能希望拆分这样的序列方便模型读取。
对于语言建模，目标是基于到目前为止我们看到的词元来预测下一个词元， 因此标签是移位了一个词元的原始序列。

假设我们有一个长为n的序列，我们之前说的是，我们要在随机的一个地方采样，每遍历一次数据，可能会有一个数据被用过很多次。所以我们我们要求的是扫一遍数据，所有的数据只用过一次。
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20240927231507.png)
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20240927231547.png)
因此我要将整个序列切成长为T的每一个序列。切好之后，每一次随机去里面取一段，这是最简单的做法。但是他会存在一个问题，如果下次还是这么做，可能会遍历不到某些数据。因为切法一定。
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20240927231758.png)
那么我们要怎么解决这个问题呢，我们每次切的时候，都要在（0-T）间随机取一个值，取一个K的话，从这个K元素开始切成这个长为T的序列，前K个数据就不要了。每一次随机在里面挑选一个序列放入mini_batch中。
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20240927232022.png)
这样的好处就是我们遍历一次数据，每一个数据就是用过一次，而不是我们在序列模型里面每一个模型会用T次X

在这个函数中，`Y` 是通过对 `X` 中每个子序列的最后一个元素进行预测生成的。具体来说，给定一个长度为 `num_steps` 的输入序列 `X`，模型的任务是预测下一个词，即 `Y` 中的元素。训练时，模型会学习输入序列与输出标签之间的关系，利用交叉熵损失等损失函数来评估预测的准确性。
```
def seq_data_iter_random(corpus, batch_size, num_steps):  #每一次我们再一个样本中间，随机采样一个长为T的序列，把这个东西做成y，做batch_size个。

    """使用随机抽样生成一个小批量子序列"""

    # 从随机偏移量开始对序列进行分区，随机范围包括num_steps-1

    corpus = corpus[random.randint(0, num_steps - 1):]

    # 减去1，是因为我们需要考虑标签

    num_subseqs = (len(corpus) - 1) // num_steps

    # 长度为num_steps的子序列的起始索引

    initial_indices = list(range(0, num_subseqs * num_steps, num_steps))

    # 在随机抽样的迭代过程中，

    # 来自两个相邻的、随机的、小批量中的子序列不一定在原始序列上相邻

    random.shuffle(initial_indices)

  

    def data(pos):

        # 返回从pos位置开始的长度为num_steps的序列

        return corpus[pos: pos + num_steps]

  

    num_batches = num_subseqs // batch_size

    for i in range(0, batch_size * num_batches, batch_size):

        # 在这里，initial_indices包含子序列的随机起始索引

        initial_indices_per_batch = initial_indices[i: i + batch_size]

        X = [data(j) for j in initial_indices_per_batch]#batch_size * T，就是number of steps的一个东西

        Y = [data(j + 1) for j in initial_indices_per_batch]#Y就是每个序列的下一个

        yield torch.tensor(X), torch.tensor(Y)
       
```
预测的过程通常涉及将输入序列 `X` 输入到神经网络（如 RNN、LSTM 或 Transformer）中，网络通过内部状态来捕捉序列中的上下文信息。最终，模型会生成一个概率分布，表示在给定输入序列的情况下，下一个词的可能性。
### 模型架构

1. **输入层**：
    
    - 输入的 `X` 是一个形状为 `(batch_size, num_steps)` 的张量，每一行代表一个长度为 `num_steps` 的序列。
2. **嵌入层（Embedding Layer）**：
    
    - 将词索引转换为稠密的向量表示，形状为 `(batch_size, num_steps, embedding_dim)`。这一步使得模型可以更好地捕捉词之间的语义关系。
3. **序列模型（RNN、LSTM、GRU 或 Transformer）**：
    
    - 这些模型通过内部状态捕捉输入序列的上下文信息。
    - **RNN**：简单的递归结构，适合短序列。
    - **LSTM/GRU**：解决长序列中的梯度消失问题，适合更长的上下文依赖。
    - **Transformer**：利用自注意力机制，处理序列间的关系，不受序列长度限制。
4. **输出层**：
    
    - 最后，通过一个全连接层将序列模型的输出映射到词汇表的大小，生成一个概率分布。这个层的输出是一个形状为 `(batch_size, vocab_size)` 的张量。
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20240927235140.png)
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20240927235147.png)
