![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20240926103836.png)
## 残差块
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20240926104555.png)
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20240926104642.png)
多加一条路，让其可以不同过某个层进入下一层。
## Resnet的细节
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20240926104808.png)
可以使用多种不同的残差快。
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20240926104854.png)
## ResNet块
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20240926113901.png)
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20240926113940.png)
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20240926113953.png)
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20240926114151.png)
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20240926115723.png)
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20240926115742.png)
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20240926115751.png)
代码解释：
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20240926115955.png)
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20240926120226.png)
## ResNet为什么能处理梯度消失，训练1000层。
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20240926121205.png)
正常情况下，如果你的模型已经训练的很好了，他的预测值-真实值的差就会很小，梯度值会很小。所以越往后面的深层，他的更新就越小。如果通过调整学习率来想让他增大的话，效果也很不好。![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20240926121426.png)
所以我们这两块都看过，y‘可以防止当原来的梯度很小的时候，不至于使得梯度消失。特别是对于底层来说，跳转可以使他的梯度不那么小。
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20240926121750.png)
靠近数据端一块的w是难以训练的，因为梯度是反向传播，所以他可以靠另一条路线直接传播过来，不需要等到主干路走完，所以一开始的时候最下面的层也能拿到比较大的梯度。