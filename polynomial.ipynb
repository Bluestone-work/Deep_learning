{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用以下三阶多项式来生成和训练测试和训练数据的标签\n",
    "$$\n",
    "y = 5^{\\frac{1}{x}} + 1.2x - 3.4\\frac{x^2}{2!} + 5.6\\frac{x^3}{3!} + \\epsilon \\quad \\text{where} \\quad \\epsilon \\sim \\mathcal{N}(0, 0.1^2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from train_ch3 import *\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_degree = 20  # 多项式的最大阶数\n",
    "n_train, n_test = 100, 100  # 训练和测试数据集大小\n",
    "true_w = np.zeros(max_degree)  # 分配大量的空间,这行代码创建了一个大小为 max_degree 的数组 true_w，初始值全部为 0。这个数组将存储生成数据时的“真实”多项式系数。\n",
    "true_w[0:4] = np.array([5, 1.2, -3.4, 5.6])#这一行设置了“真实”多项式模型的系数。\n",
    "\n",
    "features = np.random.normal(size=(n_train + n_test, 1))#这行代码从标准正态分布中随机生成 200 个输入特征，形状为 (200, 1)，作为模型的输入数据。\n",
    "np.random.shuffle(features)#这行代码将生成的 features 数据进行随机打乱，保证数据是无序的，防止任何排序引入的偏差。\n",
    "poly_features = np.power(features, np.arange(max_degree).reshape(1, -1))#这行代码生成多项式特征矩阵。np.arange(max_degree) 创建一个从 0 到 19 的数组，每个 features 中的元素被分别提升到这些指数的幂，\n",
    "#从而生成多项式特征。例如，如果 features 中某个元素是 𝑥,则对应行会是 [1,𝑥,𝑥2,𝑥3,…,𝑥19]\n",
    "np.random.shuffle(features)\n",
    "for i in range(max_degree):#这段代码循环遍历每一列，将每列的值除以阶乘，math.gamma(i + 1) 表示 𝑖!（阶乘），这是因为生成的数据使用泰勒展开近似的多项式形式。这样可以将不同阶次的特征进行缩放，避免高阶项在数值上过大。\n",
    "    poly_features[:, i] /= math.gamma(i + 1)  # gamma(n)=(n-1)!\n",
    "# labels的维度:(n_train+n_test,)\n",
    "labels = np.dot(poly_features, true_w)\n",
    "labels += np.random.normal(scale=0.1, size=labels.shape)#这行代码为每个标签值加上一个从标准正态分布中抽样的噪声，噪声的标准差为 0.1。这是为了模拟真实数据中的噪声情况，使生成的标签数据不完全符合理想的多项式函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.1028],\n",
       "         [-0.9377]]),\n",
       " tensor([[ 1.0000e+00, -1.1763e+00,  6.9184e-01, -2.7127e-01,  7.9773e-02,\n",
       "          -1.8767e-02,  3.6793e-03, -6.1828e-04,  9.0910e-05, -1.1882e-05,\n",
       "           1.3977e-06, -1.4946e-07,  1.4651e-08, -1.3257e-09,  1.1138e-10,\n",
       "          -8.7347e-12,  6.4216e-13, -4.4434e-14,  2.9037e-15, -1.7977e-16],\n",
       "         [ 1.0000e+00, -2.1724e+00,  2.3596e+00, -1.7087e+00,  9.2798e-01,\n",
       "          -4.0319e-01,  1.4598e-01, -4.5304e-02,  1.2302e-02, -2.9694e-03,\n",
       "           6.4508e-04, -1.2740e-04,  2.3063e-05, -3.8540e-06,  5.9802e-07,\n",
       "          -8.6609e-08,  1.1759e-08, -1.5027e-09,  1.8136e-10, -2.0736e-11]]),\n",
       " tensor([ -0.2782, -15.2811]))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NumPy ndarray转换为tensor\n",
    "true_w, features, poly_features, labels = [torch.tensor(x, dtype=\n",
    "    torch.float32) for x in [true_w, features, poly_features, labels]]\n",
    "\n",
    "features[:2], poly_features[:2, :], labels[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对模型进行测试和评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_loss(net, data_iter, loss):  #@save\n",
    "    \"\"\"评估给定数据集上模型的损失\"\"\"\n",
    "    metric = d2l.Accumulator(2)  # 损失的总和,样本数量\n",
    "    for X, y in data_iter:\n",
    "        out = net(X)\n",
    "        y = y.reshape(out.shape)\n",
    "        l = loss(out, y)\n",
    "        metric.add(l.sum(), l.numel())\n",
    "    return metric[0] / metric[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_shape, *args, **kwargs ) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.fl = nn.Sequential(nn.Linear(input_shape, 1, bias=False))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fl(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(\"log\")\n",
    "def train(train_features, test_features, train_labels, test_labels,\n",
    "          num_epochs=400):\n",
    "    loss = nn.MSELoss(reduction='none')\n",
    "    loss = loss.to(device)\n",
    "    input_shape = train_features.shape[-1]\n",
    "    # 不设置偏置，因为我们已经在多项式中实现了它\n",
    "    net = Net(input_shape).to(device)\n",
    "    batch_size = min(10, train_labels.shape[0])\n",
    "    train_iter = d2l.load_array((train_features, train_labels.reshape(-1,1)),batch_size)\n",
    "    test_iter = d2l.load_array((test_features, test_labels.reshape(-1,1)),batch_size, is_train=False)\n",
    "    trainer = torch.optim.SGD(net.parameters(), lr=0.01)\n",
    "    for epoch in range(num_epochs):\n",
    "        for epoch in range(num_epochs):\n",
    "            #训练\n",
    "            #print(\"----------第{}轮训练开始----------\".format(epoch + 1))\n",
    "            net.train()\n",
    "            for data in train_iter:\n",
    "                X , y = data\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                y_hat = net(X)\n",
    "                l = loss(y_hat , y)\n",
    "                trainer.zero_grad()\n",
    "                l.mean().backward()\n",
    "                trainer.step()\n",
    "            #测试\n",
    "            net.eval()\n",
    "            with torch.no_grad():\n",
    "                for data in  test_iter:\n",
    "                    X , y = data\n",
    "                    X = X.to(device)\n",
    "                    y = y.to(device)\n",
    "                    y_hat = net(X)\n",
    "                    l = loss(y_hat , y)\n",
    "        if epoch == 0 or (epoch + 1) % 20 == 0:\n",
    "            writer.add_scalar(\"train_loss1\",evaluate_loss(net, train_iter, loss),epoch + 1)\n",
    "            writer.add_scalar(\"test_loss1\",evaluate_loss(net, test_iter, loss),epoch + 1)\n",
    "    print('weight:', net[0].weight.data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Net' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpoly_features\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mn_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoly_features\u001b[49m\u001b[43m[\u001b[49m\u001b[43mn_train\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m      \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mn_train\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mn_train\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[66], line 39\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(train_features, test_features, train_labels, test_labels, num_epochs)\u001b[0m\n\u001b[0;32m     37\u001b[0m         writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss1\u001b[39m\u001b[38;5;124m\"\u001b[39m,evaluate_loss(net, train_iter, loss),epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     38\u001b[0m         writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_loss1\u001b[39m\u001b[38;5;124m\"\u001b[39m,evaluate_loss(net, test_iter, loss),epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight:\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43mnet\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Net' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "train(poly_features[:n_train, :4], poly_features[n_train:, :4],\n",
    "      labels[:n_train], labels[n_train:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
